{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport torchmetrics\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport cv2\nimport sys\nimport os\nimport glob\nfrom datasets import cityscape_dataset\nfrom models import bisenet\nfrom utils import poly_lr_scheduler, fast_hist, per_class_iou\nfrom tqdm import tqdm\nimport wandb\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--num_epochs', type=int, default=50, help='Number of epochs to train for')\n    parser.add_argument('--checkpoint_step', type=int, default=10, help='How often to save checkpoints (epochs)')\n    parser.add_argument('--validation_step', type=int, default=10, help='How often to perform validation (epochs)')\n    parser.add_argument('--dataset', type=str, default=\"Cityscapes\", help='Dataset you are using.')\n    parser.add_argument('--crop_height', type=int, default=512, help='Height of cropped/resized input image to network')\n    parser.add_argument('--crop_width', type=int, default=1024, help='Width of cropped/resized input image to network')\n    parser.add_argument('--batch_size', type=int, default=4, help='Number of images in each batch')\n    parser.add_argument('--init_lr', type=float, default=0.001, help='learning rate used for train')\n    parser.add_argument('--weight_decay', type=float, default=0.001, help='weight decay used for train')\n    parser.add_argument('--cuda', type=str, default='0', help='GPU ids used for training')\n    parser.add_argument('--use_gpu', type=bool, default=True, help='whether to user gpu for training')\n    parser.add_argument('--save_model_path', type=str, default=None, help='path to save model')\n    parser.add_argument('--image_train_path', type=str, default='Cityspaces/images/train', help='images training path')\n    parser.add_argument('--mask_train_path', type=str, default='Cityspaces/gtFine/train', help='masks training path')\n    parser.add_argument('--image_val_path', type=str, default='Cityspaces/images/val', help='images validation path')\n    parser.add_argument('--mask_val_path', type=str, default='Cityspaces/gtFine/val', help='mask validation path')\n    parser.add_argument('--wandb_key', type=str, default='', help='wandb key')\n\n    return parser.parse_args()\n\nargs = parse_args()\n\n\ndef main():\n    t_train = A.Compose([A.Resize(args.crop_height, args.crop_width, interpolation=cv2.INTER_NEAREST),])\n    t_val = A.Compose([A.Resize(args.crop_height, args.crop_width, interpolation=cv2.INTER_NEAREST),])\n\n    # Directiories\n    image_train_path = args.image_train_path\n    mask_train_path = args.mask_train_path\n    image_val_path = args.image_val_path\n    mask_val_path = args.mask_val_path\n\n    train_dataset = cityscape_dataset.CityScapes(image_train_path, mask_train_path, t_train)\n    val_dataset = cityscape_dataset.CityScapes(image_val_path, mask_val_path, t_val)\n\n    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=4, pin_memory=True)\n\n\n    # Bisenet Model\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n    bisenet_model = bisenet.BiSeNet(19, 'resnet18').to(device)\n\n    # Define loss, optimizer\n    optimizer = optim.AdamW(bisenet_model.parameters(), lr = args.init_lr, weight_decay = args.weight_decay)\n    loss_fun = nn.CrossEntropyLoss(ignore_index = 255)\n    num_epochs = args.num_epochs\n    \n    # Training\n\n    wandb.init(project=\"bisenet-no_aug-segmentation\")\n    wandb.login(key=args.wandb_key)\n\n    def train_bisenet(bisenet_model, optimizer, train_dataloader, loss_fun, device, epoch, num_epochs):\n        training_loss = 0\n        bisenet_model.train() \n        total_iou = 0\n        total_batches = 0\n\n        for batch, (image, mask) in enumerate(train_dataloader):\n            image, mask = image.to(device), mask.to(device)\n            mask = mask.type(torch.long)\n\n            optimizer.zero_grad()\n            mask_pred, _, _ = bisenet_model(image)\n            loss = loss_fun(mask_pred, mask.squeeze())\n            loss.backward()\n            optimizer.step()\n\n            \n            lr = poly_lr_scheduler(optimizer, init_lr, epoch, epochs_decay, num_epochs)\n\n            predicted = mask_pred.detach().argmax(dim=1)\n            predicted = predicted.detach().cpu().numpy().astype(int)\n            mask = mask.detach().cpu().numpy().astype(int)\n            \n            hist = fast_hist(mask.squeeze(), predicted, 19)\n            iou = per_class_iou(hist)\n            batch_iou = np.mean(iou)\n\n            total_iou += batch_iou\n            total_batches += 1\n\n            training_loss += loss.item()\n\n        mIOU = (total_iou / total_batches) * 100\n        avg_training_loss = training_loss / len(train_dataloader)\n\n        print(f\"Epoch: {epoch+1}, Training Loss: {avg_training_loss}, mIOU: {mIOU:.2f}% \\n\")\n        if epoch % args.checkpoint_step == 0 or epoch > num_epochs-2:\n            torch.save(bisenet_model.state_dict(), args.save_model_path)\n\n        \n        wandb.log({\"Training Loss\": avg_training_loss, \"Training mIOU\": mIOU})\n\n        return image, mask, predicted\n\n\n    def val_bisenet(bisenet_model, val_dataloader, loss_fun, device):\n        bisenet_model.eval()\n        val_loss = 0.0\n        total_iou = 0\n        total_batches = 0\n\n        with torch.no_grad():\n            for batch, (image, mask) in enumerate(val_dataloader):\n                image, mask = image.to(device), mask.to(device)\n                mask = mask.type(torch.long)\n                mask_pred = bisenet_model(image)\n                loss = loss_fun(mask_pred, mask.squeeze())\n                val_loss += loss.item()\n                predicted = mask_pred.detach().argmax(dim=1)\n                predicted = predicted.detach().cpu().numpy().astype(int)\n                mask = mask.detach().cpu().numpy().astype(int)\n                hist = fast_hist(mask.squeeze(), predicted, 19)\n                iou = per_class_iou(hist)\n                batch_iou = np.mean(iou)\n\n                total_iou += batch_iou\n                total_batches += 1\n\n        avg_val_loss = val_loss / len(val_dataloader)\n        mIOU = (total_iou / total_batches) * 100\n        \n        print(f'Validation Loss: {avg_val_loss:.6f}, mIOU: {mIOU:.2f}%')\n\n        \n        wandb.log({\"Validation Loss\": avg_val_loss, \"Validation mIOU\": mIOU})\n\n    for epoch in tqdm(range(num_epochs)):\n        print(f'Epoch {epoch + 1}/{num_epochs}:')\n        image, mask, mask_pred = train_bisenet(bisenet_model, optimizer, train_dataloader, loss_fun, device, epoch, num_epochs)\n         if epoch % args.validation_step == 0 or epoch > num_epoch-2:\n            val_bisenet(bisenet_model, val_dataloader, loss_fun, device)\n    \n    wandb.finish()\n\n\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"df479e6a-20ae-41d8-ae6b-38da824504c8","_cell_guid":"04239e48-8322-4167-9369-d43e6aee25bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-13T08:24:15.041696Z","iopub.execute_input":"2024-07-13T08:24:15.042196Z","iopub.status.idle":"2024-07-13T08:24:15.087734Z","shell.execute_reply.started":"2024-07-13T08:24:15.042158Z","shell.execute_reply":"2024-07-13T08:24:15.086093Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class CityScapes(Dataset):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"],"ename":"SyntaxError","evalue":"incomplete input (3614990204.py, line 1)","output_type":"error"}]}]}