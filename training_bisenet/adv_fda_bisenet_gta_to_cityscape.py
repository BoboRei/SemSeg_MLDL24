{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport torchmetrics\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport cv2\nimport sys\nimport os\nimport glob\nfrom datasets import cityscape_dataset, gta_dataset\nfrom models import bisenet\nfrom utils import poly_lr_scheduler, fast_hist, per_class_iou\nfrom tqdm import tqdm\nimport wandb\nimport discriminator\nimport fda\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--steps', type=int, default=5217, help='Number of steps to train for')\n    parser.add_argument('--iter_size', type=int, default=3, help='Number of batch slide')\n    parser.add_argument('--checkpoint_step', type=int, default=500, help='How often to save checkpoints (epochs)')\n    parser.add_argument('--validation_step', type=int, default=500, help='How often to perform validation (epochs)')\n    parser.add_argument('--dataset', type=str, default=\"Cityscapes\", help='Dataset you are using.')\n    parser.add_argument('--train_crop_height', type=int, default=720, help='Train Height of cropped/resized input image to network')\n    parser.add_argument('--train_crop_width', type=int, default=1280, help='Train Width of cropped/resized input image to network')\n    parser.add_argument('--val_crop_height', type=int, default=512, help='Val Height of cropped/resized input image to network')\n    parser.add_argument('--val_crop_width', type=int, default=1024, help='Val Width of cropped/resized input image to network')\n    parser.add_argument('--batch_size', type=int, default=4, help='Number of images in each batch')\n    parser.add_argument('--init_lr', type=float, default=0.001, help='learning rate used for train')\n    parser.add_argument('--weight_decay', type=float, default=0.001, help='weight decay used for train')\n    parser.add_argument('--cuda', type=str, default='0', help='GPU ids used for training')\n    parser.add_argument('--use_gpu', type=bool, default=True, help='whether to user gpu for training')\n    parser.add_argument('--save_model_path', type=str, default=None, help='path to save model')\n    parser.add_argument('--image_train_path', type=str, default='GTA5/images', help='images training path')\n    parser.add_argument('--mask_train_path', type=str, default='GTA5/labels', help='masks training path')\n    parser.add_argument('--image_val_path', type=str, default='Cityspaces/images/val', help='images validation path')\n    parser.add_argument('--mask_val_path', type=str, default='Cityspaces/gtFine/val', help='mask validation path')\n    parser.add_argument('--wandb_key', type=str, default='', help='wandb key')\n    parser.add_argument('--augmentation', type=str, default='', help='Which augmentation to put')\n    parser.add_argument('--Uploaded', type=str, default='False', help='To upload pre-trained weights')\n    parser.add_argument('--path_to_weights', type=str, default='', help='Path to pre-trained weights')\n    parser.add_argument('--discriminator_lr', type=str, default=0.0001, help='Discriminator lr')\n    parser.add_argument('--lambda_adv1', type=str, default=0.0002, help='Weight of discrimanator1 loss')\n    parser.add_argument('--lambda_adv2', type=str, default=0.001, help='Weight of discrimanator2 loss')\n    parser.add_argument('--lr_decay_rate', type=str, default=500, help='Lr decay rate')\n    parser.add_argument('--save_d1_path', type=str, default=None, help='path to save d1')\n    parser.add_argument('--save_d2_path', type=str, default=None, help='path to save d2')\n    \n\n    return parser.parse_args()\n\n\nargs = parse_args()\n\n\n\ndef main():\n    augmentation = args.augmentation\n    if augmentation == '':\n        t_train = A.Compose([A.Resize(args.train_crop_height, args.train_crop_width, interpolation=cv2.INTER_NEAREST),])\n    elif augmentation =='Horizontal':\n        t_train = A.Compose([\n                        A.Resize(args.train_crop_height, args.train_crop_width, interpolation=cv2.INTER_NEAREST),\n                        A.HorizontalFlip(p=0.5),\n            ], additional_targets={'mask': 'mask'})\n\n    elif augmentation == 'HSV':\n        t_train = A.Compose([\n                        A.Resize(args.train_crop_height, args.train_crop_width, interpolation=cv2.INTER_NEAREST),\n            A.OneOrOther(\n                A.Compose([\n                A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n                         ]),\n                A.NoOp(),\n    )\n            ], additional_targets={'mask': 'mask'})\n\n    elif augmentation == 'Both':\n        t_train = A.Compose([\n                        A.Resize(args.train_crop_height, args.train_crop_width, interpolation=cv2.INTER_NEAREST),\n                        A.HorizontalFlip(p=0.5),\n            A.OneOrOther(\n                A.Compose([\n                A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n                         ]),\n                A.NoOp(),\n    )\n            ], additional_targets={'mask': 'mask'})\n    \n    t_val = A.Compose([A.Resize(args.val_crop_height, args.val_crop_width, interpolation=cv2.INTER_NEAREST),])\n\n    # Directiories\n    image_train_path = args.image_train_path\n    mask_train_path = args.mask_train_path\n    image_val_path = args.image_val_path\n    mask_val_path = args.mask_val_path\n\n    train_dataset = gta_dataset.GTA(image_train_path, mask_train_path, t_train)\n    val_dataset = cityscape_dataset.CityScapes(image_val_path, mask_val_path, t_val)\n\n    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=4, pin_memory=True)\n\n    list_ = [\n        \"road\",\n        \"sidewalk\",\n        \"building\",\n        \"wall\",\n        \"fence\",\n        \"pole\",\n        \"light\",\n        \"sign\",\n        \"vegetation\",\n        \"terrain\",\n        \"sky\",\n        \"person\",\n        \"rider\",\n        \"car\",\n        \"truck\",\n        \"bus\",\n        \"train\",\n        \"motocycle\",\n        \"bicycle\"\n    ]\n     \n    init_lr = args.init_lr\n    num_steps = args.steps\n    iter_size = args.iter_size\n    lr1 = args.discriminator_lr\n    lr2 = args.discriminator_lr\n    lr_decay = args.lr_decay_rate\n    checkpoint_step = args.checkpoint_step\n    val_step = args.validation_step\n    model_path_bisenet = args.save_model_path\n    model_path_D1 = args.save_d1_path\n    model_path_D2 = args.save_d2_path\n    \n    # Bisenet Model\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n     \n    bisenet_model = bisenet.BiSeNet(19, 'resnet18').to(device)\n    \n    if args.Uploaded == 'True':\n        bisenet_model.load_state(args.path_to_weights)\n    \n    model_D1 = discriminator.FCDiscriminator(num_classes=19).to(device)\n    model_D2 = discriminator.FCDiscriminator(num_classes=19).to(device)\n        \n    # Define loss, optimizer\n    optimizer = optim.AdamW(bisenet_model.parameters(), lr = args.init_lr, weight_decay = args.weight_decay)\n    loss_fun = nn.CrossEntropyLoss(ignore_index = 255)\n    num_epochs = args.num_epochs\n \n    \n    # Discriminator optimizer and loss\n    optimizer_D1 = optim.Adam(model_D1.parameters(), lr=lr1, betas=(0.9, 0.99))\n    optimizer_D2 = optim.Adam(model_D2.parameters(), lr=lr2, betas=(0.9, 0.99))\n    bce_loss = torch.nn.BCEWithLogitsLoss()\n    \n    trainloader_iter = enumerate(train_dataloader)\n    targetloader_iter = enumerate(val_dataloader)\n\n\n    for i_iter in tqdm(range(num_steps)):\n        train_bisenet(bisenet_model, optimizer, train_dataloader, loss_fun, device, i_iter, list_, init_lr, lr_1, lr_2, trainloader_iter, targetloader_iter, val_dataloader, iter_size, model_path_bisenet, model_path_D1, model_path_D2, num_steps, lr_decay, checkpoint_step)\n        if (i_iter) % val_step == 0 or i_iter > num_steps-3:\n            val_bisenet(model_resnet18, val_dataloader, loss_fun, device, list_)\n    \n\n    \ndef train_bisenet(model_resnet18, optimizer, train_dataloader, loss_fun, device, i_iter, list_, init_lr, lr_1, lr_2, trainloader_iter, targetloader_iter, val_dataloader, iter_size, model_path_bisenet, model_path_D1, model_path_D2, num_steps, lr_decay, checkpoint_step):\n    model_resnet18.train()\n    model_D1.train()\n    model_D2.train()\n    loss_seg_value1 = 0\n    loss_adv_target_value1 = 0\n    loss_D_value1 = 0\n    loss_seg_value2 = 0\n    loss_adv_target_value2 = 0\n    loss_D_value2 = 0\n\n    total_iou = 0\n    total_batches = 0\n    epoch_loss = 0\n    iou = []\n    \n    optimizer.zero_grad()\n    init_lr = poly_lr_scheduler(optimizer, init_lr, i_iter, lr_decay_iter=lr_decay, max_iter=num_steps, power=0.9)\n    optimizer_D1.zero_grad()\n    optimizer_D2.zero_grad()\n    lr_1 = poly_lr_scheduler(optimizer_D1, lr_1, i_iter, lr_decay_iter=lr_decay, max_iter=num_steps, power=0.9)\n    lr_2 = poly_lr_scheduler(optimizer_D2, lr_2, i_iter, lr_decay_iter=lr_decay, max_iter=num_steps, power=0.9)\n\n    for sub_i in range(iter_size):\n        for param in model_D1.parameters():\n            param.requires_grad = False\n        for param in model_D2.parameters():\n            param.requires_grad = False\n\n        try:\n            _, batch = trainloader_iter.__next__()\n        except StopIteration:\n            trainloader_iter = enumerate(train_dataloader)\n            _, batch = trainloader_iter.__next__()\n\n        images, labels = batch\n        images = images.to(device)\n        labels = labels.long().to(device)\n\n        try:\n            batch_target = next(targetloader_iter)\n            target_images = batch_target[1][0]\n        except StopIteration:\n            targetloader_iter = enumerate(val_dataloader)\n            batch_target = next(targetloader_iter)\n            target_images = batch_target[1][0]\n\n        target_images = target_images.to(device)\n        images_fda = fda.FDA_source_to_target(images, target_images).to(device)\n\n        pred1, pred2 , _ = model_resnet18(images_fda)\n        \n        loss_seg1 = loss_fun(pred1, labels)\n        loss = loss_seg1 / iter_size\n        loss.backward()\n        loss_seg_value1 += loss_seg1.item() / iter_size\n        \n        # Compute training loss e mIoU\n        epoch_loss += loss_seg1.item()\n        predicted = pred1.detach().argmax(dim=1).cpu().numpy().astype(int)\n        labels = labels.cpu().numpy().astype(int)\n        \n        hist = fast_hist(labels.squeeze(), predicted, 19)\n        iou.append(per_class_iou(hist))\n        batch_iou = np.mean(iou[total_batches])\n        \n        total_iou += batch_iou\n        total_batches += 1\n        \n        # Train G with target\n        try:\n            _, batch = targetloader_iter.__next__()\n        except StopIteration:\n            targetloader_iter = enumerate(val_dataloader)\n            _, batch = targetloader_iter.__next__()\n\n        images, _ = batch\n        images = images.to(device)\n\n        pred_target1, pred_target2, _ = model_resnet18(images)\n        pred_target1 = interp_target(pred_target1)\n        pred_target2 = interp_target(pred_target2)\n\n        D_out1 = model_D1(F.softmax(pred_target1))\n        D_out2 = model_D2(F.softmax(pred_target2))\n\n        loss_adv_target1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(source_label).to(device)) \n        loss_adv_target2 = bce_loss(D_out2, torch.FloatTensor(D_out2.data.size()).fill_(source_label).to(device))\n        \n        loss = lambda_adv_target1 * loss_adv_target1 + lambda_adv_target2 * loss_adv_target2\n        loss = loss / iter_size\n        loss.backward()\n        loss_adv_target_value1 += loss_adv_target1.item() / iter_size\n        loss_adv_target_value2 += loss_adv_target2.item() / iter_size\n\n        # Train discriminator D on source\n        for param in model_D1.parameters():\n            param.requires_grad = True\n        for param in model_D2.parameters():\n            param.requires_grad = True\n\n        pred1 = pred1.detach()\n        pred2 = pred2.detach()\n\n        D_out1 = model_D1(F.softmax(pred1))\n        D_out2 = model_D2(F.softmax(pred2))\n        \n        loss_D1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(source_label).to(device))\n        loss_D2 = bce_loss(D_out2, torch.FloatTensor(D_out2.data.size()).fill_(source_label).to(device))\n\n        loss_D1 = loss_D1 / iter_size / 2\n        loss_D2 = loss_D2 / iter_size / 2\n\n        loss_D1.backward()\n        loss_D2.backward()\n\n        loss_D_value1 += loss_D1.item()\n        loss_D_value2 += loss_D2.item()\n        \n        # Train discriminator D on train\n        pred_target1 = pred_target1.detach()\n        pred_target2 = pred_target2.detach()\n\n        D_out1 = model_D1(F.softmax(pred_target1))\n        D_out2 = model_D2(F.softmax(pred_target2))\n\n        loss_D1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(target_label).to(device))\n        loss_D2 = bce_loss(D_out2, torch.FloatTensor(D_out2.data.size()).fill_(target_label).to(device))\n        \n        loss_D1 = loss_D1 / iter_size / 2\n        loss_D2 = loss_D2 / iter_size / 2\n\n        loss_D1.backward()\n        loss_D2.backward()\n\n        loss_D_value1 += loss_D1.item()\n        loss_D_value2 += loss_D2.item()\n        \n    optimizer.step()\n    optimizer_D1.step()\n    optimizer_D2.step()\n\n    if i_iter % checkpoint_step == 0 or  i_iter > num_steps-3:\n        torch.save(model_resnet18.state_dict(), model_path_bisenet)\n        torch.save(model_D1.state_dict(), model_path_D1)\n        torch.save(model_D2.state_dict(), model_path_D2)\n        print(f\"Learning rate BiSenet at iteration {i_iter+1}: {init_lr}\")\n        print(f\"Learning rate D1 at iteration {i_iter+1}: {lr_1}\")\n        print(f\"Learning rate D2 at iteration {i_iter+1}: {lr_2}\")\n\n    # Total mIoU and training loss computation\n    mIOU = (total_iou / total_batches) * 100\n    training_loss = epoch_loss / len(train_dataloader)\n\n    print(f\"Iteration: {i_iter+1}, Training Loss: {training_loss:.4f}, mIoU: {mIOU:.2f}%\")\n    if (i_iter + 1) % checkpoint_step == 0 or i_iter > num_steps-3:\n        for i in range(len(list_)):\n            print(f\"Accuracy {list_[i]}: IOU: {100 * np.mean([vector[i] for vector in iou]):.2f}%\")      \n            \n\n    def val_bisenet(bisenet_model, val_dataloader, loss_fun, device):\n        bisenet_model.eval()\n        val_loss = 0.0\n        total_iou = 0\n        total_batches = 0\n\n        with torch.no_grad():\n            for batch, (image, mask) in enumerate(val_dataloader):\n                image, mask = image.to(device), mask.to(device)\n                mask = mask.type(torch.long)\n                mask_pred = bisenet_model(image)\n                loss = loss_fun(mask_pred, mask.squeeze())\n                val_loss += loss.item()\n                predicted = mask_pred.detach().argmax(dim=1)\n                predicted = predicted.detach().cpu().numpy().astype(int)\n                mask = mask.detach().cpu().numpy().astype(int)\n                hist = fast_hist(mask.squeeze(), predicted, 19)\n                iou = per_class_iou(hist)\n                batch_iou = np.mean(iou)\n\n                total_iou += batch_iou\n                total_batches += 1\n\n        avg_val_loss = val_loss / len(val_dataloader)\n        mIOU = (total_iou / total_batches) * 100\n        \n        print(f'Validation Loss: {avg_val_loss:.6f}, mIOU: {mIOU:.2f}%')\n\n        \n        wandb.log({\"Validation Loss\": avg_val_loss, \"Validation mIOU\": mIOU})\n\n    \n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"df479e6a-20ae-41d8-ae6b-38da824504c8","_cell_guid":"04239e48-8322-4167-9369-d43e6aee25bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-13T08:24:15.041696Z","iopub.execute_input":"2024-07-13T08:24:15.042196Z","iopub.status.idle":"2024-07-13T08:24:15.087734Z","shell.execute_reply.started":"2024-07-13T08:24:15.042158Z","shell.execute_reply":"2024-07-13T08:24:15.086093Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class CityScapes(Dataset):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"],"ename":"SyntaxError","evalue":"incomplete input (3614990204.py, line 1)","output_type":"error"}]}]}