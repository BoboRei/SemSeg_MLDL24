{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport torchmetrics\nimport timm\nfrom torchvision import models\n\n\n# Context path\nclass resnet18(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.resnet18(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\nclass resnet101(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.resnet101(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\ndef build_contextpath(name):\n    model = {\n        'resnet18': resnet18(pretrained=True),\n        'resnet101': resnet101(pretrained=True)\n    }\n    return model[name]\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nclass ConvBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                               stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, input):\n        x = self.conv1(input)\n        return self.relu(self.bn(x))\n\n\nclass Spatial_path(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n\n    def forward(self, input):\n        x = self.convblock1(input)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        return x\n\n\nclass AttentionRefinementModule(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.sigmoid = nn.Sigmoid()\n        self.in_channels = in_channels\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input):\n        # global average pooling\n        x = self.avgpool(input)\n        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n        x = self.conv(x)\n        x = self.sigmoid(self.bn(x))\n        # x = self.sigmoid(x)\n        # channels of input and x should be same\n        x = torch.mul(input, x)\n        return x\n\n\nclass FeatureFusionModule(torch.nn.Module):\n    def __init__(self, num_classes, in_channels):\n        super().__init__()\n        # self.in_channels = input_1.channels + input_2.channels\n        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n        self.in_channels = in_channels\n\n        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input_1, input_2):\n        x = torch.cat((input_1, input_2), dim=1)\n        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n        feature = self.convblock(x)\n        x = self.avgpool(feature)\n\n        x = self.relu(self.conv1(x))\n        x = self.sigmoid(self.conv2(x))\n        x = torch.mul(feature, x)\n        x = torch.add(x, feature)\n        return x\n\n\nclass BiSeNet(torch.nn.Module):\n    def __init__(self, num_classes, context_path):\n        super().__init__()\n        # build spatial path\n        self.saptial_path = Spatial_path()\n\n        # build context path\n        self.context_path = build_contextpath(name=context_path)\n\n        # build attention refinement module  for resnet 101\n        if context_path == 'resnet101':\n            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n\n        elif context_path == 'resnet18':\n            # build attention refinement module  for resnet 18\n            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n        else:\n            print('Error: unspport context_path network \\n')\n\n        # build final convolution\n        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n\n        self.init_weight()\n\n        self.mul_lr = []\n        self.mul_lr.append(self.saptial_path)\n        self.mul_lr.append(self.attention_refinement_module1)\n        self.mul_lr.append(self.attention_refinement_module2)\n        self.mul_lr.append(self.supervision1)\n        self.mul_lr.append(self.supervision2)\n        self.mul_lr.append(self.feature_fusion_module)\n        self.mul_lr.append(self.conv)\n\n    def init_weight(self):\n        for name, m in self.named_modules():\n            if 'context_path' not in name:\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                elif isinstance(m, nn.BatchNorm2d):\n                    m.eps = 1e-5\n                    m.momentum = 0.1\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, input):\n        # output of spatial path\n        sx = self.saptial_path(input)\n\n        # output of context path\n        cx1, cx2, tail = self.context_path(input)\n        cx1 = self.attention_refinement_module1(cx1)\n        cx2 = self.attention_refinement_module2(cx2)\n        cx2 = torch.mul(cx2, tail)\n        # upsampling\n        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n        cx = torch.cat((cx1, cx2), dim=1)\n\n        if self.training == True:\n            cx1_sup = self.supervision1(cx1)\n            cx2_sup = self.supervision2(cx2)\n            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n\n        # output of feature fusion module\n        result = self.feature_fusion_module(sx, cx)\n\n        # upsampling\n        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n        result = self.conv(result)\n\n        if self.training == True:\n            return result, cx1_sup, cx2_sup\n\n        return result\n","metadata":{"_uuid":"df479e6a-20ae-41d8-ae6b-38da824504c8","_cell_guid":"04239e48-8322-4167-9369-d43e6aee25bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-13T08:24:15.041696Z","iopub.execute_input":"2024-07-13T08:24:15.042196Z","iopub.status.idle":"2024-07-13T08:24:15.087734Z","shell.execute_reply.started":"2024-07-13T08:24:15.042158Z","shell.execute_reply":"2024-07-13T08:24:15.086093Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class CityScapes(Dataset):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"],"ename":"SyntaxError","evalue":"incomplete input (3614990204.py, line 1)","output_type":"error"}]}]}